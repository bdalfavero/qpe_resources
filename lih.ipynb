{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e38da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from itertools import accumulate\n",
    "from time import perf_counter_ns\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix, kron, identity\n",
    "import cirq\n",
    "import openfermion as of\n",
    "from openfermionpyscf import run_pyscf\n",
    "import quimb.tensor as qtn\n",
    "from quimb.tensor.tensor_1d import MatrixProductState, MatrixProductOperator\n",
    "from convert import to_groups_of\n",
    "from error_pert import get_v2_sarray\n",
    "from qpe_trotter import v2_pauli_sum, v2_qubop\n",
    "from kcommute import get_si_sets\n",
    "from tensor_network_common import pauli_sum_to_mpo, mps_to_vector\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0010118-2ddc-404c-a1b9-75e066009eca",
   "metadata": {},
   "source": [
    "This notebook requires quantum-toolbox installed: https://github.com/jdrowland/quantum-toolbox/\n",
    "\n",
    "The following timing and validation results are run on 10 amd20 nodes with 16 GB of memory.  Comparison is made to cirq.PauliSum only.  Timing results are significantly worse on this hardware than the original notebook timing results.  This likely owes to superior memory streaming on the original (Mac Pro?) hardware.  Speedups achieved are plausibly robust provided computations remain memory-bound.  From other work I have done, AMD24 hardware remains the best option on ICER for memory bound compute, and should be constrained for cluster work of this nature.  Parallelization is implemented to demonstrate that the remaining dominant component of the computation is in the large number of expectation values which are embarassingly parallel.  Highly recommend parallelizing this differently than demonstrated in this notebook in practice by utilizing the cluster's scavenger queue to produce a very large number of cores as you scale beyond LiH.  Such an approach is approach is not friendly to the notebook format however, and as such we utilize multiprocessing.\n",
    "\n",
    "The quantum-toolbox contains a relatively diverse set of modular functions built upon symplectic data structures primarily intended for grouping Pauli operators and computing their expectation values.  Signficant performance detriments result from changing between formats and as such it is recommended to work exclusively with the symplectic form as much as possible.  Ideally all conversion happen directly to and from symplectic form without intermediate products.  This effect shouldn't inherently improve scaling, but it is likely for memory bound processes such as commutativity checks, the cirq objects induce non-linear overhead.  \n",
    "\n",
    "The toolbox contains methods for converting between and computing with different formats such as cirq, qiskit, and tensor networks.\n",
    "\n",
    "To highlight a couple of speedups:  Grouping occurs in a mere 15 ms as compared to the original 5.9s, a 393x speedup.  This speedup grows as problems become more challenging.  Serial computation of expectation values exhibits a 2.26x speedup, while the parallel version is an additional 6.5x on the expectation value computation, not quite achieving the theoretical 10x, for a relative speedup (including build time) of 4.2x.  No comparison is made to the original because for a fair comparison one would also need to parallelize the original cirq based code as well.  \n",
    "\n",
    "In general, with enough cores this computation can be driven toward the build time + grouping time.  Parallelization of the symplectic build_v2_terms is also possible should this become a bottleneck, though a little bit trickier to do.  A reasonable approach might be to produce the O(N^2) list of commutators [A,B], and then proceed to parallelize the last computation.  Combining the results of the different computations results in some overhead, as an 'abstract' sum of strings need to be combined, rather than scalars.  This can be avoided by parallelizing the expectation values at the level of the commutator also.\n",
    "\n",
    "The embarassingly parallel aspect of the expectation values still remains a non-trivial problem as you scale to a problem of the size of OWP.  The number of expectation values scales very poorly with the number of terms, something like O(M^3) where M is the number of terms, so scaling to OWP is at worst a trillion times worse.  Even with scavenger queue, the number of cores you could access in parallel are roughly 10^4, so this remains prohibitive.  Perhaps computing all of the commutators is possible, but computing their expectation values *exactly* for a general state seems unlikely.  For grouping, the number of expectation values scales linearly with M, so this remains feasible to do.  Perhaps you could still do this exactly for a relatively unentangled state, such as HF, or by somehow truncating the number of expectation values required, though this seems unlikely to meet your goals.  \n",
    "\n",
    "\n",
    "At the LiH problem size, I think you will be always worse off doing it this way rather than sparse matrix methods, but as problem size grows, I expect the sparse methods to eventually be worse, perhaps comparable for water with maximal parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d3aad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF energy: -7.76736213574856\n",
      "FCI energy: -7.784460280031227\n",
      "Hamiltonian has 12 qubits and 631 terms.\n"
     ]
    }
   ],
   "source": [
    "molec = \"LiH\"\n",
    "basis = \"sto-3g\"\n",
    "n_elec = 4\n",
    "geometry = of.chem.geometry_from_pubchem(molec)\n",
    "multiplicity = 1\n",
    "molecule = of.chem.MolecularData(\n",
    "    geometry, basis, multiplicity\n",
    ")\n",
    "molecule = run_pyscf(molecule, run_scf=1, run_fci=1)\n",
    "print(f\"HF energy:\", molecule.hf_energy)\n",
    "print(f\"FCI energy:\", molecule.fci_energy)\n",
    "hamiltonian = molecule.get_molecular_hamiltonian()\n",
    "hamiltonian_qubop = of.transforms.jordan_wigner(hamiltonian)\n",
    "hamiltonian_psum = of.transforms.qubit_operator_to_pauli_sum(hamiltonian_qubop)\n",
    "\n",
    "nq = of.utils.count_qubits(hamiltonian_qubop)\n",
    "nterms = len(hamiltonian_qubop.terms)\n",
    "print(f\"Hamiltonian has {nq} qubits and {nterms} terms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "740593cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted_inds = ['k0', 'k1', 'k2', 'k3', 'k4', 'k5', 'k6', 'k7', 'k8', 'k9', 'k10', 'k11']\n"
     ]
    }
   ],
   "source": [
    "qs = cirq.LineQubit.range(nq)\n",
    "hamiltonian_mpo = pauli_sum_to_mpo(hamiltonian_psum, qs, 100)\n",
    "dmrg = qtn.DMRG(hamiltonian_mpo, bond_dims=15)\n",
    "converged = dmrg.solve()\n",
    "if not converged:\n",
    "    print(\"DMRG did not converge.\")\n",
    "ground_state = dmrg.state\n",
    "ground_state_vec = mps_to_vector(ground_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ecbf313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 42 groups.\n",
      "Elapsed time: 5.92668e+09 ns\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter_ns()\n",
    "groups = get_si_sets(hamiltonian_psum, nq)\n",
    "end_time = perf_counter_ns()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"There are {len(groups)} groups.\")\n",
    "print(f\"Elapsed time: {elapsed_time:4.5e} ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b7f159",
   "metadata": {},
   "source": [
    "## Sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "147359f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got eps2=0.009780710988690324 in 3.70797e+09 ns\n"
     ]
    }
   ],
   "source": [
    "# Use the code from the paper.\n",
    "start_time = perf_counter_ns()\n",
    "group_qubops = to_groups_of(groups)\n",
    "sparse_frag_ops = []\n",
    "# Convert group operators to sparse matrices.\n",
    "# We must make sure the matrices have all the same size.\n",
    "# If a matrix is not big enough, tensor it with I on the right.\n",
    "for op in group_qubops:\n",
    "    nq_op = of.utils.count_qubits(op)\n",
    "    op_sparse = of.linalg.get_sparse_operator(op)\n",
    "    if nq_op != nq:\n",
    "        eye_diff = identity(2 ** (nq - nq_op), dtype=\"complex\", format='csc')\n",
    "        new_op = kron(op_sparse, eye_diff, format=\"csc\")\n",
    "        sparse_frag_ops.append(new_op)\n",
    "    else:\n",
    "        sparse_frag_ops.append(op_sparse)\n",
    "v2_sparse = get_v2_sarray(sparse_frag_ops)\n",
    "eps2 = np.vdot(ground_state_vec, v2_sparse @ ground_state_vec).real\n",
    "end_time = perf_counter_ns()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Got eps2={eps2} in {elapsed_time:4.5e} ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7531ff",
   "metadata": {},
   "source": [
    "## `cirq.PauliSum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2450fb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got eps2=(0.00978075160645751-1.2363017708628819e-20j) in 1.84036e+12 ns\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter_ns()\n",
    "group_psums = [sum(group) for group in groups]\n",
    "v2_psum = v2_pauli_sum(group_psums)\n",
    "qubit_map = {q: i for i, q in enumerate(qs)}\n",
    "eps2_psum = v2_psum.expectation_from_state_vector(ground_state_vec, qubit_map)\n",
    "end_time = perf_counter_ns()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Got eps2={eps2_psum} in {elapsed_time:4.5e} ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d776774f",
   "metadata": {},
   "source": [
    "## `of.QubitOperator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = perf_counter_ns()\n",
    "group_qubops = to_groups_of(groups)\n",
    "v2_op = v2_qubop(group_psums)\n",
    "qubit_map = {q: i for i, q in enumerate(qs)}\n",
    "eps2_qubop = v2_psum.expectation_from_state_vector(ground_state_vec, qubit_map)\n",
    "end_time = perf_counter_ns()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Got eps2={eps2_qubop} in {elapsed_time:4.5e} ns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pd39d274q4",
   "metadata": {},
   "source": [
    "## quantum-toolbox (symplectic representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1y2goqjp5k6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Hamiltonian: 630 terms, 12 qubits\n",
      "Grouped into 42 groups in 0.015 s (native symplectic format)\n"
     ]
    }
   ],
   "source": [
    "# Use quantum-toolbox for native symplectic representation\n",
    "import sys\n",
    "sys.path.insert(0, '../quantum-toolbox')\n",
    "from qtoolbox.core.pauli import PauliString\n",
    "from qtoolbox.core.hamiltonian import Hamiltonian\n",
    "from qtoolbox.converters.openfermion_bridge import from_openfermion\n",
    "from qtoolbox.grouping import sorted_insertion_grouping\n",
    "from itertools import accumulate\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Convert OpenFermion QubitOperator to quantum-toolbox Hamiltonian\n",
    "n_qubits_qt = of.utils.count_qubits(hamiltonian_qubop)\n",
    "terms = [from_openfermion(term, coeff, n_qubits_qt)\n",
    "         for term, coeff in hamiltonian_qubop.terms.items() if term]  # skip identity\n",
    "ham = Hamiltonian(terms)\n",
    "print(f\"Loaded Hamiltonian: {ham.num_terms()} terms, {ham.num_qubits()} qubits\")\n",
    "\n",
    "# Group using quantum-toolbox's SI\n",
    "start_group = perf_counter_ns()\n",
    "group_collection = sorted_insertion_grouping(ham)\n",
    "time_grouping = perf_counter_ns() - start_group\n",
    "\n",
    "sym_groups = [list(g.paulis) for g in group_collection.groups]\n",
    "print(f\"Grouped into {len(sym_groups)} groups in {time_grouping/1e9:.3f} s (native symplectic format)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8r2bcphgnio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_commutator_sum(a_terms, b_terms):\n",
    "    result = []\n",
    "    for ai in a_terms:\n",
    "        for bj in b_terms:\n",
    "            if not ai.commutes_with(bj):  \n",
    "                p = ai.multiply(bj)       \n",
    "                p.coeff *= 2              \n",
    "                result.append(p)\n",
    "    return result\n",
    "\n",
    "def apply_pauli_to_state(x_bits, z_bits, n_qubits, state):\n",
    "    dim = len(state)\n",
    "    result = state.copy()\n",
    "\n",
    "    x_bits_of, z_bits_of = 0, 0\n",
    "    for q in range(n_qubits):\n",
    "        if x_bits & (1 << q):\n",
    "            x_bits_of |= (1 << (n_qubits - 1 - q))\n",
    "        if z_bits & (1 << q):\n",
    "            z_bits_of |= (1 << (n_qubits - 1 - q))\n",
    "\n",
    "    if z_bits_of:\n",
    "        indices = np.arange(dim)\n",
    "        parity = np.zeros(dim, dtype=np.int8)\n",
    "        for b in range(n_qubits):\n",
    "            if z_bits_of & (1 << b):\n",
    "                parity ^= ((indices >> b) & 1).astype(np.int8)\n",
    "        result *= (1 - 2*parity)\n",
    "\n",
    "    if x_bits_of:\n",
    "        result = result[np.arange(dim) ^ x_bits_of]\n",
    "\n",
    "    y_bits = x_bits & z_bits\n",
    "    if y_bits:\n",
    "        result *= (1j) ** bin(y_bits).count('1')\n",
    "\n",
    "    return result\n",
    "\n",
    "def build_v2_terms(sym_groups):\n",
    "    nterms = len(sym_groups)\n",
    "    sums_l2r = list(accumulate(sym_groups, lambda a, b: a + b))\n",
    "    sums_r2l = list(reversed(list(accumulate(reversed(sym_groups), lambda a, b: a + b))))\n",
    "    sums_r2l.append([])\n",
    "\n",
    "    v2_terms = []\n",
    "    for i in range(1, nterms):\n",
    "        V1 = fast_commutator_sum(sums_l2r[i-1], sym_groups[i])\n",
    "        for t in fast_commutator_sum(V1, sums_r2l[i+1]):\n",
    "            t.coeff *= -1/3\n",
    "            v2_terms.append(t)\n",
    "        for t in fast_commutator_sum(V1, sym_groups[i]):\n",
    "            t.coeff *= -1/6\n",
    "            v2_terms.append(t)\n",
    "    return v2_terms\n",
    "\n",
    "def compute_expectation_sequential(v2_terms, psi, n_qubits):\n",
    "    eps2 = 0.0\n",
    "    for t in v2_terms:\n",
    "        p_state = apply_pauli_to_state(t.x_bits, t.z_bits, n_qubits, psi)\n",
    "        eps2 += (t.coeff * np.vdot(psi, p_state)).real\n",
    "    return eps2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3u7tuefmzqs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantum-toolbox (sequential):\n",
      "   V2 terms:     7,513,732\n",
      "   eps2 =        0.0097807516\n",
      "   Build time:   83.774 s\n",
      "   Exp. time:    729.712 s\n",
      "   Total time:   813.487 s\n",
      "   Error vs sparse: 8.83e-16\n",
      "   ✓ Validation passed\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter_ns()\n",
    "v2_terms = build_v2_terms(sym_groups)\n",
    "time_build = perf_counter_ns() - start_time\n",
    "\n",
    "start_exp = perf_counter_ns()\n",
    "eps2_symplectic = compute_expectation_sequential(v2_terms, ground_state_vec, nq)\n",
    "time_exp = perf_counter_ns() - start_exp\n",
    "\n",
    "end_time = perf_counter_ns()\n",
    "elapsed_ns = end_time - start_time\n",
    "\n",
    "error = abs(eps2 - eps2_symplectic)\n",
    "print(f\"quantum-toolbox (sequential):\")\n",
    "print(f\"   V2 terms:     {len(v2_terms):,}\")\n",
    "print(f\"   eps2 =        {eps2_symplectic:.10f}\")\n",
    "print(f\"   Build time:   {time_build/1e9:.3f} s\")\n",
    "print(f\"   Exp. time:    {time_exp/1e9:.3f} s\")\n",
    "print(f\"   Total time:   {elapsed_ns/1e9:.3f} s\")\n",
    "print(f\"   Error vs sparse: {error:.2e}\")\n",
    "assert error < 1e-8, f\"Results don't match! Error: {error}\"\n",
    "print(\"   ✓ Validation passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "nfsbmp4w2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_expectation_batch(args):\n",
    "    terms_data, psi, n_qubits = args\n",
    "    dim = len(psi)\n",
    "    total = 0.0\n",
    "    for x_bits, z_bits, coeff in terms_data:\n",
    "        result = psi.copy()\n",
    "        x_bits_of, z_bits_of = 0, 0\n",
    "        for q in range(n_qubits):\n",
    "            if x_bits & (1 << q):\n",
    "                x_bits_of |= (1 << (n_qubits - 1 - q))\n",
    "            if z_bits & (1 << q):\n",
    "                z_bits_of |= (1 << (n_qubits - 1 - q))\n",
    "        if z_bits_of:\n",
    "            indices = np.arange(dim)\n",
    "            parity = np.zeros(dim, dtype=np.int8)\n",
    "            for b in range(n_qubits):\n",
    "                if z_bits_of & (1 << b):\n",
    "                    parity ^= ((indices >> b) & 1).astype(np.int8)\n",
    "            result = result * (1 - 2*parity)\n",
    "        if x_bits_of:\n",
    "            result = result[np.arange(dim) ^ x_bits_of]\n",
    "        y_bits = x_bits & z_bits\n",
    "        if y_bits:\n",
    "            result = result * ((1j) ** bin(y_bits).count('1'))\n",
    "        total += (coeff * np.vdot(psi, result)).real\n",
    "    return total\n",
    "\n",
    "def compute_expectation_parallel(v2_terms, psi, n_qubits, n_workers=None):\n",
    "    if n_workers is None:\n",
    "        n_workers = max(1, cpu_count())  \n",
    "    terms_data = [(t.x_bits, t.z_bits, t.coeff) for t in v2_terms]\n",
    "    \n",
    "    batch_size = len(terms_data) // n_workers + 1\n",
    "    batches = [(terms_data[i:i+batch_size], psi, n_qubits) \n",
    "               for i in range(0, len(terms_data), batch_size)]\n",
    "    \n",
    "    with Pool(n_workers) as pool:\n",
    "        results = pool.map(compute_expectation_batch, batches)\n",
    "    return sum(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exhxdenarz5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Build time:   83.538 s\n",
      "quantum-toolbox (parallel, 9 workers):\n",
      "   eps2 =        0.0097807110\n",
      "   Time:         111.085 s\n",
      "   ✓ Validation passed\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter_ns()\n",
    "v2_terms = build_v2_terms(sym_groups)\n",
    "time_build = perf_counter_ns() - start_time\n",
    "print(f\"   Build time:   {time_build/1e9:.3f} s\")\n",
    "n_workers = 9\n",
    "print(f\"quantum-toolbox (parallel, {n_workers} workers):\")\n",
    "\n",
    "start_time = perf_counter_ns()\n",
    "eps2_parallel = compute_expectation_parallel(v2_terms, ground_state_vec, nq, n_workers)\n",
    "time_parallel = perf_counter_ns() - start_time\n",
    "\n",
    "error_par = abs(eps2 - eps2_parallel)\n",
    "print(f\"   eps2 =        {eps2_parallel:.10f}\")\n",
    "print(f\"   Time:         {time_parallel/1e9:.3f} s\")\n",
    "assert error_par < 1e-8, f\"Results don't match! Error: {error_par}\"\n",
    "print(\"   ✓ Validation passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e35412e",
   "metadata": {},
   "source": [
    "## MPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00daf35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_groups_mpo(groups: List[List[cirq.PauliString]], qs: List[cirq.Qid], max_bond: int) -> List[MatrixProductOperator]:\n",
    "    \"\"\"Convert groups from SI into a list of MatrixProductOperators.\"\"\"\n",
    "\n",
    "    mpos: List[MatrixProductOperator] = []\n",
    "    for group in groups:\n",
    "        group_psum = sum(group)\n",
    "        group_mpo = pauli_sum_to_mpo(group_psum, qs, max_bond=max_bond)\n",
    "        mpos.append(group_mpo.copy())\n",
    "    return mpos\n",
    "\n",
    "def mpo_add_compress(\n",
    "    mpo_a: MatrixProductOperator, mpo_b: MatrixProductOperator,\n",
    "    compress: bool = False, max_bond: int = 100\n",
    ") -> MatrixProductOperator:\n",
    "    \"\"\"Add two MPOs, optionally compressing them.\"\"\"\n",
    "\n",
    "    mpo_sum = mpo_a + mpo_b\n",
    "    if compress:\n",
    "        mpo_sum.compress(max_bond=max_bond)\n",
    "    return mpo_sum\n",
    "\n",
    "\n",
    "def mpo_commutator(mpo_a: MatrixProductOperator, mpo_b: MatrixProductOperator) -> MatrixProductOperator:\n",
    "    \"\"\"Take the commutator of two MPOs.\"\"\"\n",
    "\n",
    "    return mpo_a.apply(mpo_b) - mpo_b.apply(mpo_a)\n",
    "\n",
    "\n",
    "def zeros_mpo(nsites: int, phys_dim: int=2) -> MatrixProductOperator:\n",
    "    \"\"\"Returns an MPO corresponding to a matirx of all zeros.\"\"\"\n",
    "\n",
    "    def fill_fun(shape):\n",
    "        return np.zeros(shape, dtype=complex)\n",
    "    \n",
    "    return MatrixProductOperator.from_fill_fn(fill_fun, L=nsites, bond_dim=1, phys_dim=phys_dim)\n",
    "\n",
    "\n",
    "def get_v2_contrib_mpo(fragments_list: List[MatrixProductOperator], psi: MatrixProductState, max_bond: int) -> float:\n",
    "    \"\"\"Calculate eps2 when the fragments are MPOs.\"\"\"\n",
    "\n",
    "    max_mpo_sites = max([len(mpo.tensor_map) for mpo in fragments_list])\n",
    "\n",
    "    frags_len = len(fragments_list)\n",
    "    frag_sums_l2r = list(accumulate(fragments_list, lambda a, b: mpo_add_compress(a, b, True, max_bond)))\n",
    "    temp = reversed(fragments_list)\n",
    "    frag_sums_r2l = list(accumulate(temp, lambda a, b: mpo_add_compress(a, b, True, max_bond)))\n",
    "    frag_sums_r2l = list(reversed(frag_sums_r2l))\n",
    "    frag_sums_r2l.append(zeros_mpo(max_mpo_sites))\n",
    "    frag_combs_V1_v2 = [(frag_sums_l2r[i-1], fragments_list[i], frag_sums_r2l[i+1]) for i in range (1, frags_len)]\n",
    "    eps2 = 0\n",
    "    for i,j,k in frag_combs_V1_v2:\n",
    "        V1_term = mpo_commutator(i, j)\n",
    "        term1 = psi.H @ psi.gate_with_mpo(mpo_commutator(V1_term, k))\n",
    "        term2 = psi.H @ psi.gate_with_mpo(mpo_commutator(V1_term, j))\n",
    "        eps2 += - term1 *1/3 - term2 *1/6\n",
    "    return eps2.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "143f4afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_mpo_bond = 5\n",
    "start_time = perf_counter_ns()\n",
    "mpo_fragments = to_groups_mpo(groups, qs, max_mpo_bond)\n",
    "end_time = perf_counter_ns()\n",
    "elapsed_time_convert = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3be18c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/.venv/compare/lib/python3.13/site-packages/autoray/autoray.py:81: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return func(*args, **kwargs)\n",
      "/Users/benjamindalfavero/.venv/compare/lib/python3.13/site-packages/autoray/autoray.py:81: RuntimeWarning: overflow encountered in matmul\n",
      "  return func(*args, **kwargs)\n",
      "/Users/benjamindalfavero/.venv/compare/lib/python3.13/site-packages/autoray/autoray.py:81: RuntimeWarning: invalid value encountered in matmul\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got eps2=0.009230812660028556.\n",
      "Conversion time: 1.23026e+09, calculation time: 8.41664e+10.\n",
      "Error 5.40575e-04\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter_ns()\n",
    "eps2_mpo = get_v2_contrib_mpo(mpo_fragments, ground_state, max_mpo_bond)\n",
    "end_time = perf_counter_ns()\n",
    "elapsed_time_calculate = end_time - start_time\n",
    "\n",
    "print(f\"Got eps2={eps2_mpo}.\")\n",
    "print(f\"Conversion time: {elapsed_time_convert:4.5e}, calculation time: {elapsed_time_calculate:4.5e}.\")\n",
    "eps2_err = abs(eps2 - eps2_mpo)\n",
    "print(f\"Error {eps2_err:4.5e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18234c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
